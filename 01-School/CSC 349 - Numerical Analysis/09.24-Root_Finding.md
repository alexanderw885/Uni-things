We want to find a value of $x$, such that $f(x)=0$.
There's two main families of function to consider:
- **Polynomials**: We will find both real and complex roots for polynomial functions
- **Non-polynomial**: algebraic and "transcendental functions" such as $e^{-x}-x=0$. We'll focus on finding real roots

Numerical methods will be iterative. We won't find the *exact* roots, but the longer we run the program, the closer and closer we cant get.

# Bisection method
We can use the bisection method to calculate the root of any function that's continuous on the interval $[x_l,\space x_u]$ , for which $(x_l)$ and $f(x_u)$ have different signs.
This means if one end of the interval is negative, the other end is positive, and the function is continuous, we can use the bisection method.
It's a simple algorithm. Just calculate the value of the function at the interval's midpoint, and treat that as one of the endpoints. This will half the size of your interval with each step.
Once the interval is small enough, you can stop the interval to see your root. Note this will only find one root in the interval, and if you run it multiple times it will always find the same root.

One problem with this method is that you need to at least start with an initial interval. Sometimes it's not clear where a root would be

# Newton-Raphson method
The rough idea is to treat the function as linear at your initial guess, and iterate until you get the precision needed.
we start with the initial guess $x_0$. We assume it's on a straight line with a slope of $f'(x_0)$. We can iterate with the following equation:
$$x_{i+1}=x_i-\frac{f(x_i)}{f'(x_i)}$$
![[Pasted image 20250926103645.png]]
This method can converge much faster than the bisection method.

How do we want to calculate error? We don't just calculate $|f(root)|<\epsilon$. Instead, we calculate $|1-\frac{x_0}{root}|<\epsilon$
- $x_0$ is the initial guess
- root is out most recent iteration of $x$
- epsilon is our choice of allowed error

While it can converge quite fast, there are a number of cases where it does not:
![[Pasted image 20251001105817.png]]

In general, the further your initial guess from the root, the longer it'll take to converge.
To get around this, it can be worth it to start with the bisection method to get "close enough", then switch to the Newton-Raphson method to very quickly get an accurate final result.

# Secant method
Another problem with the Newton-Raphson method is that you need to know the derivative. Instead of using the derivative, we use an approximation
$$x_{i+1}=x_i-\frac{f(x_i)(x_{i-1}-x_i)}{f(x_{i-1})-f(x_i)}$$
This one requires no knowledge of what the function is, we just need to be able to call the function.
Do note that this relies on $two$ previous iterations, not just one.
In general, this method is faster than the bisection method, but slower than Newton-Raphson.
