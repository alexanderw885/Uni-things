now that we have our [intermediate representation](06.25-Intermediate_Code_Generation), we need to use it to generate machine code. In our case, our target architecture is JVM, but it could be for any architecture.
Along with targeting an architecture, the generated code usually will have to follow some published standards:
- Application binary interface: describes intended uses for specific registers
- Key: assists interoperation tools that make up or use the compiler stages

Code gen must accomplish three things:
1. Instruction selection: choose which instructions on the target machine will be used for each IR operation
2. Instruction scheduling: choose the order for target operations to be executed
3. Register allocation: Decide which values should be in any register at each point in the program.

These three tasks are traditionally done separately, but they all depend on each other. You can't think of them as different steps, completing one and then the next.

### Java ecosystem
 While it was originally just for Java, the JVM is not exclusive to it. Kotlin, Scala, Groovy, and Clojure for
 example all target JVM. ![[Pasted image 20250715125851.png]]

**Java classfiles**
Java classfiles `.class` are a combination of:
- Configuration info
- Bytecode implementation of methods
- String table/constant pool
![[Pasted image 20250715131120.png]]
![[Pasted image 20250715131137.png]]
![[Pasted image 20250715131153.png]]

### Jasmin
We'll be using an assembler that converts a textual representation of JVM opcode into an actual `.class` file.
### Pattern matching
For our assignment, we'll map each IR instruction to one pattern of JVM instructions. Most of our instructions will have a one-to-one match, but in general it can be 2 or 3 IR instructions as one sequence, matched to some bytecode sequence.

![[Pasted image 20250716123940.png]]
Note that the arguments for `iload` and `istore` aren't constants, but temporary value identifiers.
Also note we're putting values on the stack, not focusing as much on register allocation for this assignment.

As an aside, note that for variables T0-T3, there's a shorter form of bytecode `istore_0`, `iload_3`, and so on. We don't have to use them, but they're nice to have.

For a small optimization, look at the next piece of bytecode:
![[Pasted image 20250716124424.png]]
Note that after performing the addition, we pull the result from the stack and into variable 13. We then put it back onto the stack for the next addition. We could instead just leave it on the stack, and go straight to `iload 14`.
```
iload 0
iload 2
iadd

iload 14
iadd
istore 15
```
However, what if we need T13 again? This will only work if this is the last instance of T13.
if some value Tx won't be used again, we might denote it in notes as Tx$^{\text{last}}$ 

Let's say we have some longer IR sequences, consisting of multiple IR instructions. How do we decide what pattern to match? for RISC architectures, the greedy approach of always choosing the longest pattern is quite good. However, it's not quite that easy for CISC architectures.

Let's look as some ceelish and it's IR:
![[Pasted image 20250716125639.png]]
let's do a fairly naive conversion to bytecode:
![[Pasted image 20250716125742.png]]
![[Pasted image 20250716125755.png]]
For `PRINTLN`, let's go through it step by step
1. We get a reference to the `PrintStream` and put it on the stack
2. we push the integer onto the stack
3. we convert the integer into a String
4. we call `println`. It takes the first element on the stack as the argument, and the second element as the target. This means it takes the string, and prints it into the `PrintStream`
### Optimization
We always want to make the code better, though in general we can't know if any given solution is "optimal". We want to focus on improving the following, *without changing the behavior of the code*:
- runtime
- memory usage
- power consumption
- minimize branches
runtime is generally the focus of optimization, but it's not the only target.
We can minimize branches with [loop unrolling](05.22-Software_Optimization_Techniques), and by reducing function calls.
Power consumption has gotten more important in recent years, partly due to int prevalence of laptops and mobile phones.

As an aside, the difference between IR and code optimization:
- IR optimizations are applicable to all architectures
- code optimization with be specific for each given architecture.

There are three types of optimization:
#### Local optimization
applied to a single basic block
**Common subexpression elimination**
	when duplicate calculations are performed, calculate it once and use the result multiple times
**Copy propagation**
	if we were to have `T3 := T4`, and `T3` is used *before* the definitions of either temp are changed, we can skip the assignment and just use `T4`
	instead of: `T3 := T4; T5 := T3`,
	we use `T5 := T4`.
**Dead code elimination**
	if a variable is assigned to, but not used until assigned to again, we can skip the first assignment.
	If a variable is dead at assignment, we don't need to assign to it.
**Arithmetic simplification, Strength reduction**
	covered in [SENG440](05.22-Software_Optimization_Techniques). Replace slower operations with faster operations, or multiple faster operations.
	look into book "Hacker's Delight" by Henry Warren
**Constant folding**
	if an expression can be computed statically, then you should do so. Don't add constants in the code, do it yourself and write the sum in directly
**Loop unrolling**
	also in [SENG440](05.22-Software_Optimization_Techniques). Perform multiple iterations in each loop. The code is more complex and takes up more space, but there's less jumps and potentially better pipelining.
#### Global optimization
Applied to the control-flow graph for a full function.
**Common subexpression elimination**
	![[Pasted image 20250718125610.png]]
**Code hoisting**
	can save time, but mostly saves space.
	If an expression is calculated in multiple branches (if/else statement), calculate it before the branch, and use the result in both branches.
**Code motion out of loops**
	similar to hoisting, but mostly improves time.
	if an expression is evaluated to the same value in every iteration of a loop, calculate it before the loop and use the result every iteration.
**Loop fusion**
	once again in  [SENG440](05.22-Software_Optimization_Techniques). if there's multiple loops that don't interact with each other, merge them together. This saves on jumps and comparisons.
#### Interprocedural optimization
applied to CFGs across multiple functions. These analysis are more complex and more expensive, but can lead to very large payoffs.
**Function inlining**
	jumping to and from a function takes time. Inlining functions will increase the size of the program, but less control-flow transfers will speed up the program.
	Can often be written as a function, but declared as inline.
	Watch out though, too much can have impacts on cache hits/misses
**Escape analysis**
	like ownership in [rust](03.19-Rust). If a variable is used outside of a function, or "escapes" the function. We need to store it on the heap. But, if we don't need the variable after a function, we can allocate it on the stack, deallocating it once the function ends.
	For example, if an object is declared in a function and not returned or referenced to by another object, it can be allocated on the stack. It won't be needed once the function returns, so it doesn't need to be on the heap.
**Alias analysis**
	generally applies to the entire program.
	determines if memory locations can be accessed by multiples names throughout different parts of the program. If two references never alias, there can be optimizations.

