How can you define a programming language? 
This is a hard problem, and no single framework can solve it. Instead of that, we break it into layers, each of which has suitable language for the task it's in charge of. 
It can be compared to "letters","words", and so on:
- let's start with the set of "letters" in a language. 
- Each "word" is a sequence of letters, it has a start, end, and an order. 
- each "sentence" is a sequence of words


Lexical analysis:
convert the input data into tokens, to make the parser's job easier. This is hard to make by hand, so we use a lexer generator. In this class, we use `ANTLR`, but there's also `lex`, `flex`, and `Jflex`. They take in an input file that specifies the structure, and converts the input data into tokens.

**Note:** "language" in this course uses the CSC330 formal definition, the set of all strings belonging to a language. L('a')={"a"}, or 
L("a$^+$")  = {"a", "aa", "aaa", ... }

![[Pasted image 20250513125407.png]]
Note that s$^?$ means either 0 or 1 of s.
Examples:

| In Ceelish                         | Regex                           |
| ---------------------------------- | ------------------------------- |
| print keyword                      | `"print"`                       |
| any `id`                           | ([a-z]\|\_)([a-z]\|[0-9]\|\_)\* |
| any `floatconstant`                | [0-9]+\.[0-9]+                  |
| a string in python of only letters | ('[0-9]\*')\|("[0-9]\*")        |

We can construct an automaton to recognize the language, like in CSC320. How do we do this?
Start by building a nondeterministic finite automata (NFA). We need to do this so we can use $\epsilon$ transitions.
Once we've made an NFA, how do we convert it into a DFA?
We create a DFA, where each state is a combination of states in the NFA. The total set of states in the DFA is the power set of states in the NFA

**Homework**: Make DFA for a\*(a|b)aa

The lexer, or scanner, transform the input letters/characters into a sequence of tokens. These tokens get passed into the parser for semantic analysis. The parser is like a DFA, or finite automaton.

Most of the time, parsing uses a context-free grammar,  though they can be context-sensitive in some cases.
It outputs a form of intermediate representation, though often not *the* IR. it's usually a syntax tree, that then needs to be walked through in order to get the actual IR.

Grammars are often written using [Extended Backus-Naur Form](01.15-Syntax_Semantics)

Just like discussed in CSC330, there can be cases where there can be multiple syntax trees for the same parse string. If we do a leftmost derivation or rightmost derivation, the final result may be different. How might we prevent this?
Precedence. 
![[Pasted image 20250520123543.png]]
Note that in this case, addition and subtraction much happen higher up in the tree, with multiplication and division lower. This enforces order of operations/precedence, and constrains the result so that no matter if you use leftmost or rightmost derivation, you always get the same result.

**Ambiguity in grammar**
`if E1 then if E2 then S1 else S2
what does the else connect to?
python examples:
```python
# solution 1
if E1:
	if E2:
		S1
	else:
		S2

# solution 2
if E1:
	if E2:
		S1
else:
	S2
```
the general procedure in this case would be solution 1. *Match an else statement with the closest if statement*.
In general, we can reflect this by adding a `matched` and `unmatched` to the grammar:
![[Pasted image 20250520130048.png]]
However, there are still cases where this does not work.

### Parsers
#### Top Down
start at the root of derivation tree, and work down.
Backtracking may be required, and we continue until the "fringe", all the leaves, match the input string in an in-order traversal.
Top down parsers **Cannot** handle left-recursion, if doing a leftmost derivation.
![[Pasted image 20250521123552.png]]
`<expr> := <expr> + <term>` causes the parser to get stuck in a recursion loop. We can solve this by 
transforming the grammar. Start with the following grammar fragment:
```
<foo> ::= <foo>a
        | b
```
we can write this grammar as b+a\*
assuming both a and b do not start with `<foo>`, we can transform it to:
```
<foo> ::= b<bar>
<bar> ::= a<bar>
	   | epsilon
```

Let's look at a real-world example:
```
<term> ::= <term> * <factor>
		 | <term> / <factor>
		 | <factor>
```
`<term>` is like `<foo>` in the example
`* <factor>` and `/ <factor>` are `a`
and `<factor>` is beta

We can transform it into: 
```
<term> ::= <factor> <term'>
<term'> ::= * <term'>
		  | / <term'>
		  | epsilon
```

Another method would be to add rightmost recursion instead. This does solve the problem, but it add right-associativity, which might not be desirable.


The parser we've talked about need backtracking. This works, but surely there's a better way, and there is! By adding lookahead, we can reduce or remove backtracking.
- LL(1): left-to-right scan, leftmost derivation, one token of lookahead
- LR(1): left to right scan, *rightmost* derivation, one token lookahead.

Let's look at LL(1).
let's say we have the rule A -> a | b.
What we want to be able to do is tell right away whether a or b is the correct choice.
let's define FIRST(a): FIRST(a) is the set of all possible leftmost tokens derived from a. If we choose a, whatever the first character in that string is, it's in FIRST(a).
the key property we need is the following:
$FIRST(a)\cap{}FIRST(b)=\emptyset$ 

Can we convert any grammar into this format? It's actually undecidable if any given grammar has an equivalent that follows these rules. However, for the languages we're interested in, this should always be possible.
### Recursive Descent Parsers
Given a grammar that can support one-symbol lookahead, we can write a non-backtracking recursive parser using a stack.

Recursive parsers work, but they aren't perfect. 
### Non-recursive Predictive Parsing
These use table-driven parsers. These table-driven-parsers take inputs from a parsing table, the current stack state, and tokens from the parser/lexer.
Ideally, we just create the language grammar, put that into a parser generator, and then the generator will create the parser tables. ANTLR4 is the parser generator we'll be using. 
We start with the starting non-terminal in the stack. 
then, we use the next token from the parser and the terminal or non-terminal on the stack to look up a value in the parser tables. 
Next, put the terminals and non-terminals from the table onto the stack.

- Look at top of stack:
	- if it's a terminal, compare to the next token
		- if they match, pop off top of stack and advance to the next token. 
		- Else, error
	- if it's a non terminal, use that and next token to look up value in parser table
		- if new terminals and non-terminals are found, push them onto stack *in reverse order* (end-of-file counts as a token, `$`)
		- Else, error
	- if the stack is empty:
		- if the next token is end-of-file, we're done!
		- Else, error
- Repeat until done


how do parse tables look?
each row is a non-terminal
each column is a token.

How do we build these parse tables? First, we'll need to calculate FIRST(a) for all non-terminals *and* terminals. For this build, we'll use $\epsilon$ instead of NULLIBLE.
FIRST(X):
- if X is a terminal, then FIRST(X) = {X}
- if X -> $\epsilon$, add $\epsilon$ to FIRST(X)
- If X -> $Y_1Y_2$...$Y_k$:
	- add FIRST($Y_1$) - {$\epsilon$} to FIRST(X)
	- if FIRST($Y_1$) contains $\epsilon$, add FIRST($Y_2$)
	- Repeat for $Y_2$, $Y_3$, ...$Y_k$, until no more additions to FIRST(X) can be made.
- if all tokens $Y_1...Y_k$ contain $\epsilon$, add $\epsilon$ to FIRST(X)
This algorithm may have to go through multiple iterations before getting the complete set

#### FIRST and FOLLOW
We'll also need FOLLOW(A). FOLLOW(A) is **not** like FIRST. While FIRST is about the terminals that A resolves into, FOLLOW is the set of all terminals that come *after* the non-terminal A. This means that FOLLOW(A) is not determined by what A produces, but instead by the non-terminals that have A on the right hand side of one of their rules.

More of a definition:
FOLLOW(A) = $\beta$ for all possible values for all $S\rightarrow\alpha{}A\beta$, where $\alpha, \beta$ can be $\epsilon$, and A is a non-terminal.
Examples: 
$E'\rightarrow+E$:
$\alpha=+$, $A=E$,$\beta=\epsilon$

$T=FT'$:
$\alpha=F$,$A=T'$,$\beta=\epsilon$
or
$\alpha=\epsilon$,$A=F$,$\beta=T'$

FOLLOW():
- Initialize FOLLOW(A) to $\emptyset$
- if A is a terminal, you're done!
- Add end of file $ to FOLLOW(S) , where S is the start of the tree
- if A -> $\alpha{}B\beta$:
	- put FIRST($\beta$) - $\epsilon$ into FOLLOW(B), **not** A
	- if $\beta = \epsilon$, meaning A -> $\alpha$B or FIRST($\beta$) contains $\epsilon$, add FOLLOW(A) into FOLLOW(B). if B is the end of A, anything that follows A will also follow B
- Repeat these steps for every production rule.
- You may have to iterate though all steps multiple times.
___
#### Parse Tables
Now that we have the FIRST and FOLLOW sets for each non-terminal, along with all the production rules, we can construct the parse table.

Input: Grammar G
Output: Table M

1. for all production A -> $\alpha$:
	1. for all a in FIRST($\alpha$), add A -> $\alpha$ to M\[A, a\]
	2. if $\epsilon$ is in FIRST($\alpha$), 
		1. for all b in FOLLOW($\alpha$), add A -> $\alpha$ to M\[A,b]
		2. if EOF $ is in FOLLOW(A), add A->$\epsilon$ to M\[A,$\]
2. Set all undefined cells in M to `error`
If there's any cells with multiple entries, the grammar is not in LL(1)

### Error recovery in predictive parsing
what do we do when there is no production in a cell we access in the parse table? We want to keep going if possible, in order to give the programmer as much useful information as possible.

lets look at an example with a simple grammar:
1. E -> E + T | T
2. T -> T ￼￼* F | F
3. F -> (E) | id

4. E -> TE'
5. 