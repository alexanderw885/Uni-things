What is a compiler? 
It's something that translates higher level languages to lower level languages. This does **not** always mean to bytecode or assembly, just to some lower level language.

**Side note**: when compiling with `gcc`, use the `-S` argument to produce assembly.

How do we do this translation?
The compiler is actually a pipeline, going through multiple steps in order to eventually get the result.
The front end of the compiler is everything before machine code is generated. It includes scanning, parsing, analyzing semantics, and then code generation.
The back end is everything after code generation. This is optimization, register allocation, and more.
![[Pasted image 20250509122906.png]]

### [Scanner](05.13-Lexical_Analysis_&_Parsing)
A scanner, a.k.a lexer or lexical analyzer, converts the text input into tokens. It simplifies the written code to make it easier for the parser to read. This can be combined with the parser, but traditionally isn't.
### [Parser](05.13-Lexical_Analysis_&_Parsing)
A parser takes the scanner's output and builds the abstract syntax tree
### [Semantic Analyzer](05.13-Lexical_Analysis_&_Parsing)
The semantic analyzer runs the tree through a number of checks to ensure that it's valid. Type analysis, scope analysis, ... By the end, you know you have meaningful code.
### [Intermediate Representation Generator](06.25-Intermediate_Code_Generation)
The last step of the front end, the IR generator traverses the abstract syntax tree (AST) and produces IR code. There can be a lot of freedom in how the IR is formatted. 

Instead of creating a separate compiler for each language and each target architecture, we instead compile to an intermediate representation, and then separately to each architecture. This way, when we make a new language/architecture, we only need to make one new compiler
### Machine Independent Optimization
Before producing machine code, we can try to improve the performance of the IR. This includes, but is not at all limited to:
- **common subexpression elimination (CSE)**: if an expression or subexpression is performed multiple times, perform it just once and save it as a variable.
- **operator strength reduction**: replaces expensive operations with cheaper ones. Cheaper might mean less power, fewer cycles, or a combination of both. This is especially important for optimizing loops
- **code motion**: (loop-invariant code motion:) If there's a loop in the code, and there's an expression that always produces the same result, code motion moves that expression before the loop and stores it as a variable.
A challenge in writing this optimization is recognizing these chances for optimization.

### Code Generation
Finally, code generation is assembling the code for the targeted architecture. This step can be somewhat straight-forward compared to other steps, though not always.
### Register Allocation
Up to this point, we've made the assumption that we have unlimited registers in the target CPU. As this obviously isn't the case, we now need to decide what goes into what registers, and when.
The reason we made that assumption is that it makes the assembly code significantly easier to generate.
This is a difficult problem, but now is solved using graph colouring
### Peephole Optimization
peephole optimization is fairly straight forwards, it replaces obvious redundancies with more efficient code. It only looks at small numbers of commands at a time, not the whole program.
![[Pasted image 20250509131124.png]]
### Instruction Scheduling
The goal of instruction scheduling is to reorder instructions in such a way to increase speed without impacting the result. It's very heavily dependent on the machine it's run on.
unrolling loops, reorder instructions to better support pipelining. Branches cause issues with pipelining, so they especially need to be taken into account.