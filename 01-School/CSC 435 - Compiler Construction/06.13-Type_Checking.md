Compared to parsing and AST construction, this is much less elegant.

### Types of Typing
**Strong typing**
	when an operation can only be applied with the correct datatypes. The compiler guarantees that if an operation proceeds, it has the correct datatype.
**Weak typing**
	when it's up to the programmer to ensure operands have the correct type. One language can have both string and weak typing in different areas
**dynamic typing**
	checks for operand datatypes are performed at runtime
**Static typing**
	checks for operand types are performed at compile time. Goes with strong typing.
![[Pasted image 20250613130532.png]]


while weak vs. strong typing is a choice made by the language's developer, dynamic vs. static is a choice of the language's implementer. This means that type checking can be static with one interpreter, and dynamic in the other.

___
Now, we bring in [attributes](06.11-Attribute_Grammars).
In general, attributes can be intrinsic or computed
- Intrinsic attributes are solely dependent on the node itself. This could be the name of an identifier, or the line number.
- Computed attributes are what we've talked about so far. It's the synthesized and inherited attributes. The datatype of an expression is a synthesized attribute, and the [symbol table](06.03-Scopes_and_Symbol_Tables) is inherited.

We can perform type checking using synthesized attributes. For example, in an if statement, the conditional must be a boolean. Or, in an array call `a[3]`, we need to make sure the index is an integer.

We can implement this type checking with a [visitor pattern.](05.30-Abstract_Syntax_Trees) ![[Pasted image 20250618124445.png]]
Or, we could use a table-based approach. This is harder to initially set up, but scales much better and is easier to maintain for larger languages.
![[Pasted image 20250618124540.png]]

**Error handling in type checking**
Often if we have an expression with multiple operations, if there's an error it'll cause multiple error messages. How can we get around this, for clearer error messages? There's two strategies:
- if you reach an error, abandon checking that unit, and keep going afterwards.
- set the datatype of an undeclared expression to error object. Now the function returns on object of type error, and keep going from there.

**Type widening**
when you assign a variable of one type to a variable of another type. A good example is:
```c
int i = 6;
float fl = int;
```
the integer was "widened" into a float. In general that's the term used for when you move to a larger bit length, but not exclusively.

**Overloading**
Most of the time, the `+` operator works on both floating point and integers. This is known as operator overloading. Despite being very similar to us, integer and floating point arithmetic are very different operations.

The other type of overloading is function overloading. This is when you have multiple functions with the same name, but take different parameters. A great example of this is constructor methods in java. They're all called the same thing, but take different arguments and can have entirely different behavior.

**Type synthesis**
Much like synthesized attributes, the type is determined by the types of its sub-expressions. For example, `a + b` would be determined by the types of `a` and `b`. 

### **Type inference**
the other form of type checking, with type synthesis.
We determine the type of `x` by looking at all the places where it's used. For example, if we have a function `func(int val)`, and we call `func(x)`, we can tell that `x` must be an int.

### Polymorphism
polymorphism is when there's code that can be executed with arguments of different types.
parametric polymorphism has parameters or type variables, limiting what types can be used.

**Unification**
let's look at the following code:
```
fun length(x) = 
  if null(x) then 0
  else length (tail(x)) + 1
```
what types can this take? Any sort of list, regardless of what the list is of.

for some function definition with one argument:
`fun id1 (id2) = E`
We'll create type variables $\beta, \gamma$.
We map $\beta\rightarrow\gamma$ to `id1`
We map $\beta$ to `id2`
since we passed in `x` as an argument `id2`, we know we know `x`=$\beta$.
...
![[Pasted image 20250624124543.png]]

**Substitution**
Unification depends on finding a mapping from type variables to type expressions. We can do this by finding a suitable substitution.
the result of applying the Substitution S onto some type t is S(t)
Two types unify if there exists some substitution S such that S(t1)=S(t2). We can create a table for t, and S(t):
![[Pasted image 20250624125740.png]]
For this example, as it's small, we can even draw out a graph for it:
![[Pasted image 20250624125810.png]]
The symbol left of the colon is the type, and the number to the right is the equivalence class. Any two vertices that share the same number are equivalent.

Here's some code to see if you can unify two types  `m`, `n`:
![[Pasted image 20250624130118.png]]