nodes with numerical attributes always split into 2 branches, with the form $f_1>constant$
## Entropy
![[Pasted image 20241009163443.png]]
for this example, we'd consider 9 different splits of R, and six splits of L: ![[Pasted image 20241009163522.png]]
![[Pasted image 20241009163600.png]]
We test the entropy after each of these 15 different splits, and find that we get the lowest entropy when we place the split at $L=1.5$, with an entropy of $0.63$
![[Pasted image 20241009164023.png]]
if $L<1.5$, we always predict "No"
We recur on all points where $L>1.5$, and eventually end up with the following graph and tree:
![[Pasted image 20241009164157.png]]
An alternative method than using entropy is using GINI
## GINI
$$GINI(p_1,...,p_m)=1-\sum_{i=1}^np_i^2$$

![[Pasted image 20241009164436.png]]

GINI doesn't need any logarithms, and has all-around lower entropy on two-value probabilities. Both methods are popular, and often end up with the same final trees.

# Regression Trees
instead of having yes/no at each leaf, a regression tree has real numbers.

instead of using entropy, we use variance $\sigma^2$, and for each leaf we'll use the mean $\mu$.

We stop when the variance at a node is less than some value, or when there's few enough instances in each the node. That node then becomes a leaf
