neural networks need vectors, what if we want to use them to predict words?

We could try one-hot encoding, but then you'd need a column for every single available word, so the matrix could get very large when looking at larger documents.

Another problem is that similar words need to be near each other in the vector space. We need to find a way to keep local context for each word. This is where you use Word2Vec.

two goals:
1. convert a high-dimension one-hot representation into a lower-dimensional representation
2. maintain word context/meaning.

### Skip-gram
a n-gram is a group of n words, in a "window". the $n$ is the window size.
ex: the cat is here in 3-grams
- the cat is
- cat is here

skip refers to how large on the left and right of a word a window should be created.
![[Pasted image 20241030173105.png]]
the skip in this image is 2, and the gram is 5.

![[Pasted image 20241030173401.png]]


Our output layer is a matrix, and we use this matrix as a lookup table. it has in this case 300 columns, and one row for each input word. Each word correlates to one row in our lookup table.
Similar words are close to each other in the vector space