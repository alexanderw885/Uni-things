# Linear Regression
tries to find line of best fit
- create line
- measure error from line to each point
	- this error distance is squared to remove negative values
- adjust line to minimize error
this last step uses the gradient descent algorithm
- adjust each parameter, check if error is reduced. If not, remove change. If there is less error, keep the change and move on to another value
- Gradient descent also works on polynomial regression

# Decision trees
not the greatest model
find the most important attribute, sort based off of that
then the next most important, sort based off of that, and so on
can make many decision trees to help try to increase accuracy

# Logistic Regression
given a set of points in a true/false state, find a way to tell if a new point will be true or false
- use a line or plane to try and separate the true values from false
- count number of values on the wrong side
- move the separator in order to minimize number of error points
Again, this uses the gradient descent algorithm
easy to generalise to higher dimension data sets

This can also use multiple lines/planes/functions to separate the fields
This method is often used with neural networks

When multiple lines would be applicable, the line of best fit will be as evenly spaced between true and false sides

the functions used to not need to be linear, other shapes may be a better fit

# K-Means Clustering
how can a computer determine best clusters?
- create points in random places, sort datapoints by which of the new random points they are closest to
- now there's clusters for each new point we created. Move the new points to the centroid of their cluster
- repeat these steps until adjusting the centroids no longer change location