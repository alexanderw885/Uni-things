### Instance-based learning
makes predictions for new cases by comparing them to existing examples

### Model-based learning
most of this course is focused on model-based learning
builds a model of existing examples, then uses this model to make predictions

### Linear function
$$y=\theta_0+\theta_1x$$
the thetas are unknown, we need to use a cost minimisation function to solve for them

### Model selection and training a model 
selection - choosing a model and fully specifying its architecture
once you've selected the model to use, you need to train it
training a model - using an algorithm to find model parameters with best fit (finding $\theta$)

## Challenges
### bad data
the data could be to small a sample, it takes a large amount of data to train an algorithm
the data could also be a poor representation of the set

before using the data, you need to clean it.
- handle outliers, maybe remove or correct them
- exclude values with missing values, or use median values to fill missing values

the data set may have too many features
- having too many features can hinder model performance
- not having relevant features leads to inaccurate results
- you need to identify and extract relevant features from the data sets
### bad algorithms
overfitting training data
- using a too high degree polynomial might strongly overfit the training data, instead of generalising to accurately predict new data
- the model might fail to generalise to other data points
- this can be solved with more data points, but that's often not feasible
![[Pasted image 20240909164807.png]]


Regularisation can prevent overfitting
- restricting possible $\theta$ values
- in the linear example, this could be setting $\theta_1$ to 0 only allows the algorithm to adjust $\theta_0$, only allowing vertical movement
**It's very important to strike a good balance of fitting training data and ensuring generalisation**

**Underfitting**
when the model is too simple for the data

solvable with a more complex model, or by lowing the regularisation hyperparameters

## Hyperparameters
the parameters of the algorithm, not of the model
must be set before training
tuning is very important

### Hyperparameter tuning and model selection
holdout validation
- create validation set from training data
- train multiple models with different hyperparameters on the training set
- then train the best model on the full training set