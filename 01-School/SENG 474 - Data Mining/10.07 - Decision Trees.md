Popular, able to see why the algorithm made each choice. Not many machine learning algorithms allow this.
![[Pasted image 20241009154659.png]]
Start at root, each internal node is a branch. Once it gets to a leaf, that is the prediction

## How to choose attributes for each node?
choose an attribute that splits the data into pure(dull) subsets. We want to separate the dataset into categories with as many similar results as possible. We do this with entropy
#### Entropy
lets say we have to represent letters with bits

| Letter | Probability | Binary |
| ------ | ----------- | ------ |
| A      | 0.25        | 00     |
| B      | 0.25        | 01     |
| C      | 0.25        | 10     |
| D      | 0.25        | 11     |
But what if the probabilities aren't consistent?

| Letter | Probability | Binary |
| ------ | ----------- | ------ |
| A      | 0.5         | 0      |
| B      | 0.25        | 10     |
| C      | 0.125       | 110    |
| D      | 0.125       | 111    |
how many bits on average?
$$0.5(1)+0.25(2)+0.125(3)+0.125(3)=1.75$$
On average, this lets us use less bits per symbol

How do we find the smallest possible number of average bits? The equation found for this is called entropy
$$entropy(p_1,...,p_m)=-p_1log_2(p_1)-...-p_mlog_2(p_m)$$
**Note**: doesn't have to be $log_2$, using $log_{10}$ or $ln$ will give the same result.

lets graph this for two values $p_1,p_2$, where $p_1+p_2=1$

![[Pasted image 20241009155842.png]]
as the values get closer in probability, entropy increases. As p1 approaches 1 or 0, entropy decreases.

**Note:** sometimes when calculating entropy, we have to compute $0log(0)$, which normally isn't defined. Convention for this application is to say this equals 0

### Back to Trees
![[Pasted image 20241009155959.png]]
We choose the feature with the lowest entropy.
temperature and windy appear to have balances yes/no values, so they have high entropy

Outlook has 3 values, ("Sunny","Rainy","Overcast"). What you do is calculate the entropy for each of the three categories, and take the **weighted** average. Weight by the number of values in each category
![[Pasted image 20241009160606.png]]

after calculating entropy for each feature:
- outlook: 0.693
- Temperature: 0.744
- Humidity: 0.788
- Windy: 0.892

Outlook is the lowest, so we use outlook for our node.

Now, we recur this formula, calculating entropy for each other feature for each branch off the node we already calculated. Eventually, we hit a node that's all one value, so we call that a leaf node and go down another branch. This eventually gets us a full tree
![[Pasted image 20241009161001.png]]

#### Noise
What do we do when we can't split everything perfectly?
![[Pasted image 20241009161205.png]]
we can't give a perfect value, but we can assign a leaf a probability. In this case, we could give that node "No", with a probability of $2/3$.
#### Numerical values
We can create branches based on a value being less than, or greater than a value. We try to keep numerical features to only producing two children
#### Missing values
when a sample is missing a value, we send it down all branches that would depend on it.

### Pruning
it's very easy to overfit data by creating too many branches, we need to prune to prevent this.
- pre-pruning: accept some entropy, once a branch produces less than a certain amount of entropy, do not split it any further
- post-pruning: grow the entire tree, and then simplify branches that don't cause significant changes to entropy

# Random Forests
Build thousands of decision trees, select a subset of rows and columns for each decision
Then, when predicting, put new sample into every tree and choose the most common prediction. Many computations, but accurate