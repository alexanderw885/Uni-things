Connects to `N03-classification_eval.ipynb`
true positive rate TPR:
$$\frac{\text{true positives}}{\text{true+false positives}}$$
false positive rate FPR:
$$\frac{\text{false positives}}{\text{true + false positives}}$$


We can graph this in ROC space:
![[Pasted image 20240916171916.png]]
D is the best. It got every possible true positive, and did not get a single false positive.
The closer to D the better
A may get less true positives than B, but it also gets fewer false positives. A is not better than B, B is not better than A, which classifier you want depends on the situation
C is random. On the diagonal 
line, it's 50/50 on whether a true guess is correct or not.
E, when guessing true, is wrong more often often then right. Anything below the diagonal line is a bad classifier. It's wrong more often than not, so if you flip its predictions it'll be right more often than not

### Curves in ROC space

![[Pasted image 20240916172744.png]]
You can get a ROC curve by changing the threshold in a classifier
the ideal ROC curve has as much area below it as possible, called AUC (Area Under Curve)
Bigger AUC is better

### Terms
**Sensitivity**:  $TP/P$
- same as TPR and Recall.

**Specificity:** $TN/N$

Doctors prefer to increase sensitivity, increasing false positives but increasing the odds of true positives as well.
Better to wrongly think someone has cancer than completely miss someone that actually does