### Conditional probability
P(sunny | windy) = 0.8
is the same as
P(if Wind=windy, then Weather=sunny) = 0.8,
where Wind=windy and Weather=sunny are random variables.
Remember this from probability class
$$P(a|b)=\frac{P(a\wedge{}b)}{P(b)}$$
we can do algebra, for fun
$$P(a\wedge{}b)=P(a|b)P(b)$$
and it's commutative!
$$P(b\wedge{}a)=P(b|a)P(a) = P(a\wedge{}b)$$
from this, we can get Bayes' rule:
$$P(a|b)=\frac{P(b|a)P(a)}{P(b)}$$
___
#### Example: Breathalysers
we know three things:
- 5% of the time when the driver is sober, is falsely shows they're drunk
- when someone is drunk, a breathalyser is always correct
- 1/1000 people are drunk

given the breathalyser shows drunk, what are the odds the person is drunk?
$d$=drunk
$b$=breathalyser says drunk
what we want to know: 
P(drunk, given breathalyser goes off)
$P(d|b)=P(b|d)*P(d)/P(b)$
$P(-d|b)=P(b|-d)*P(-d)/P(b)$

lets set $1/P(b)=\alpha$
$$P(d|b)=\alpha{}P(b|d)P(d)$$
$$P(-d|b)=\alpha{}P(b|-d)P(-d)$$

we know that:
$P(d)=0.001$
$P(-d)=0.999$
$P(b|d)=1$
$P(b|-d)=0.05$

now, we can simplify the above equations:
$P(d|b)=\alpha(1)(0.001)$
$P(-d|b)=\alpha(0.05)(0.999)$

since $P(d|b)+P(-d|b)=1$
we can make the formula
$$\alpha(0.001+(0.05)(0.999))=1$$
$$\alpha=19.63$$
so we get our answer:
$$P(d|b)=0.02$$

## More variables
**Note:** in math, $a\wedge{}b$ is the same as $a,b$. '$\wedge$' is the same as ','

 $c$ is the class, and $e_i$ are the evidence variables

![[Pasted image 20241009172841.png]]
that last step is called the "naive assumption", and that's why the classifier we'll be looking at is called "Naive Bayes".
# Naive Bayes
$$P(c|e_1,...,e_n)=\alpha{}P(e_1|c)...P(E_n|c)P(c)$$
![[Pasted image 20241009173438.png]]
what do we think about a rainy, cool, high-humidity, windy day? We look at the probability for both playing and for not playing
![[Pasted image 20241009173616.png]]
![[Pasted image 20241009173912.png]]
the $1/P(E)$ is $\alpha$, it's the normalization constant
### Normalization constant
for this example, P(playing)+P(not playing)=1, we use that fact to solve for alpha

$$\alpha(0.0053+0.0206)=1$$
$$\alpha=1/(0.0259)$$
Now we can answer the initial question:
$$P(Play=yes | E)=\frac{0.0053}{0.0259}=20.5$$
#### zero-frequency problem
what if an attribute value doesn't occur in every class value? We remedy this by adding one to every attribute
![[Pasted image 20241009174354.png]]
most libraries will automatically do this
#### Missing values
in training, that instance will not be included in the frequency count for that attribute-class combination
in classification, that attribute is just not used
#### Numeric Attributes
We make the assumption that attributes have a normal distribution. Now we use data such as the probability-density function, and sample mean\variance to make predictions.
$$f(x|class)=\frac{1}{\sigma\sqrt{2\pi}}e^{\frac{(x-\mu)^2}{2\sigma^2}}$$
![[Pasted image 20241016165825.png]]
We want to guess for the following:

| Outlook | Temp | Humidity | Windy |
| ------- | ---- | -------- | ----- |
| sunny   | 66   | 90       | true  |
f(Temp=66|play=true)

sample mean temp = 73
sample var temp = 38

we math our an to an answer:
![[Pasted image 20241016170254.png]]

##### Is Naive Bayes a good classifier?
it works surprisingly well, even if the data isn't entirely independent!
It also works quite well even if you don't have very much data, which can be a large issue in gathering data in the real world. More advanced methods tend to require much larger datasets.