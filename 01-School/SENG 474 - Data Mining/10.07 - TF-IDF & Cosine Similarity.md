How do we tell what words are important in documents?

## TF-IDF

TF-IDF stands for:
	term frequency - inverse document frequency

**Term Frequency**
$$TF(t,d)=\frac{\text{number of times term t occurs in document d}}{\text{Total number of terms in document d}}$$

if a term is not in the document, it's represented with 0

example:
```
banana
apple
blueberry
bananna
blueberry
```
banana = 2/5
apple = 1/5
blueberry = 2/5

**inverse Document Frequency**
$$IDF(t) = log\frac{1+n}{1+df(t)}+1$$
where n is the number of documents in the collection, and $df(t)$ = number of documents with the term t.

If only a few of the $n$ documents contain $t$, the IDF will be very large. In $t$ is in most of the documents, it will approach 0 and be very small

we can then put these two equations together to get the TF-IDF
$$TFIDF(t,d)=TF(t,d)*IDF(t)$$

In practice, we don't consider "stop words" (and, is, an, the, of,...), as they aren't important to the subject of a document

now, we have a TF-IDF value for each word in each document. We put those together to make a n-dimensional vector for each word, where $n$ is the number of documents.
In implementation, the vectors are very sparse, that should be taken into consideration when storing them.

## Cosine Similarity

$$cos(d,e)=\frac{d*e}{(d*d)*(e*e)}$$
where $d,e$ are vectors. This value will be greater when the two vectors are more similar, and smaller when they're different.
It measures angles, not distance, which means it's still relevant even in higher dimensions