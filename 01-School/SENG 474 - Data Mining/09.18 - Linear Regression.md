# With One Variable

notation:
- $m$: number of training instances
- $x$: input variable/feature
- $y$: output variable / target variable
- $(x,y)$: generic training example
- $(x^i,y^i)$: $i^{th}$ training example

| price (x) | size(y) |
| --------- | ------- |
| 2104      | 460     |
| 1416      | 232     |
| 1534      |         |
$x^1=2104$, for example

h maps from x to y, so $h(x)=y$. h, stands for hypothesis, is the model

h is linear, so we represent it with a line
$$h_\theta(x)=\theta_0+\theta_1x$$
we want to learn the best values for thetas.

### Cost function
typically denoted by:
$$J(\theta_0,\theta_1)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^i)-y^i)^2$$
finds the error of each prediction compared to the actual value, squares to ensure the value is always positive.

from this we can get the mean squared error, MSN
$$min_\theta{}(J(\theta_0,\theta_1))$$

to simplify, we can set $\theta_0=0$
now that everything is only dependent on one variable, it's easy to visualise in 2d.
map $h_\theta(x)$ with respect to x, you want a perfect 45 degree diagonal line.
you can also graph $J(\theta_1)$ with respect to $\theta_1$, find a value where $J(\theta_1)=0$ or as close to it as possible. This means the MSN is as minimal as possible.

the algorithm should use gradient descent to find the least value of $J(\theta_1)$ where $\theta_0=0$


in 3d, using both thetas, it's harder to visualise in 2d, but can be done with levels sets or in 3d. Again, the algorithm uses gradient descent in order to find the least value for J, or $min_\theta(J(\theta_0,\theta_1))$

### Gradient Descent

you want to minimize $J(\theta_0,\theta_1)$
change $\theta_0,\theta_1$ in opposite direction of gradient, found with the derivative of $J$. Repeat this until it eventually converges at a local minimum.
how far should it go at each step? This is the learning rate $\alpha$, a hyperparameter scalar.
an alpha that's too small will take too long, and can't be too big since it could overshoot the minimum

**Note:** you need to always change $\theta_0$ and $\theta_1$ at the same time, don't change one, update, then change the other.

$$\frac{\partial{}J}{\partial{}\theta_0}=\frac{1}{2m}\sum_{i=1}^m(\theta_0+\theta_1x^{(i)}-y^{(i)})^2$$
$$=\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)})$$

$$\frac{\partial{}J}{\partial{}\theta_1}=\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)})*x^{(i)}$$
note the only difference is that the derivative with respect to $\theta_1$ is you multiply it by $x^{(i)}$.

So, the final formulas to repeat until convergence are:
$$-\theta_0:=\theta_0-\alpha{}\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)})$$
$$-\theta_1:=\theta_1-\alpha{}\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)})*x^{(i)}$$
remember, calculate both, then apply both at the same time.

# With Multiple variables
- n: number of features
- x: vector of input features in training example
- $x_j$ is feature j in training example
- $x^{(i)}$ is i-th training example

![[Pasted image 20240918172820.png]]
in this example, $x^{(2)}=[1415,3,2,40,232]$
$x^{(2)}_3=2$
note that when implementing in python, start counting at 0 instead of 1

$$h_\theta(x)=\theta_0+\theta_1x_1+...+\theta_nx_n$$
you can add dummy value $x_0=1$ to get:
$$h_\theta(x)=\theta_0x_0+\theta_1x_1+...+\theta_nx_n$$
but that's just for convenience, it's not too important

cost function is the same.
$$J(\theta_0,\theta_1,...,\theta_n)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^i)-y^i)^2$$

and the gradient computation is the same as before as well, but with more variables.

$$-\theta_j:=\theta_j-\alpha{}\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)})*x_j^{(i)}$$
# GD Vectorization
instead of loops in loops, vectorize to do multiple operations at once

![[Pasted image 20240918173816.png]]
this way, you can calculate every derivative at the same time, instead of one at a time.
each.

This can be further simplified to get the result:
![[Pasted image 20240918173931.png]]
note that in the rightmost equation, both $\theta$ and $y$ are vectors, the only scalar is $\frac{1}{m}$. 

finally, repeat until convergence
$$-\theta:=\theta-\alpha\frac{1}{m}X^T(X\theta-y)$$
# Normal Equations
another way to find the best $\theta$ vector, analytically.
every differentiable function has a minimum point where the derivative/gradient becomes 0.
So, we take the gradient vector used above, set it to 0, and then solve for theta

remember: $\theta, y$ are both vectors, $X$ is a matrix
$$X^T(X\theta-y)=0$$
$$\theta=(X^TX)^{-1}X^Ty$$
once simplified, this becomes the normal equation
```python
X = np.array([  
[1, 90, 1],  
[1, 80, 3],  
[1, 90, 2],  
[1, 70, 8]  
])  
y = np. array([  
[50],  
[60],
[55],
[70]
])
# @ is matrix multiplication
theta= np.linalg.inv(X.T @ X) @ X.T @ y  
theta
```
# Which to use?
Gradient descent needs to iterate
needs to mess with $\alpha$
good choice when n is large
easy to generalise

Normal equations don't need to iterate or find a good $\alpha$
challenging to compute $(X^TX)^{-1}$
hard to do for a large n