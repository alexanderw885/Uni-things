Logistic regression is a method to estimate the probability that an instance belongs to a specific class.

if est. probability > threshold:
	output 1
else:
	output 0

The threshold is often 50%

### Estimating Probabilities
logistic regression calculates weighted sum of all input features, similarly to linear regression.
However, instead of outputting the final result, it outputs the logistics of the result
$$\hat{p}=h(\theta)=g(x\theta)$$
where g is defined as $exp(z)$, or:
$$g(z)=\frac{1}{1+e^{-z}}$$
domain: $(-\infty, \infty)$
range: $(0,1)$
this $g$ is the logistic, or sigmoid function

what is $\hat{p}$?
$$\hat{p}=P(y=1|x;\theta)$$
that's prob. y=1, given x, parameterised by theta


Now we can, given instance $x$, define $\hat{p}=h_\theta(x)$ as:
$\hat{y}$ = 1 if $x\theta\geq0$
$\hat{y}$ = 0 if $x\theta\lt0$

the line $x\theta$ is the decision boundary
### Example
$$h_\theta=g(\theta_0+\theta_1x_1+\theta_2x_2)$$
lets say we found $\theta=[-3,1,1]$
![[Pasted image 20240925165205.png]]

you can see the line $z$ mentioned above, and how it is the boundary between predicting if an instance is in the set or not

# Training and Cost Function
![[Pasted image 20240925165534.png]]
This is what we want, -ln(p) growl large when p is close to 0, and log(1-p) grows large when 0 gets close to 1
![[Pasted image 20240925165647.png]]
this is what we want, higher cost for bad predictions, and we want to minimize the cost

we calculate the overall cost as the average over all training instances

luckily, the cost function is always convex, so using gradient descent will always get us the global minimum cost. How do we get our partial derivative?
$$\frac{\partial}{\partial\theta_j}J(\theta)=\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$$
This is the exact same as the equation for linear regression, though $h_\theta()$ is different. This can easily be modified in a python function

we just need to repeat the following until theta stops changing:
$$\theta_j:=\theta_j-\alpha{}\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)})x_j^{(i)}$$

two variations:
- Stochastic GD: take one instance at a time, not the whole sum
- Mini-Batch GD: take a mini-batch at a time, not the whole batch

### Vectorization
Just like linear regression, we want to vectorize the process to speed it up.
$$-\theta:=\theta-\alpha\frac{1}{m}X^T(h_\theta(X)-y)$$
remember, everything but alpha is a vector or matrix. $h_\theta$ is being applied on a matrix, so it applies to each row separately

# Non-Linear boundaries
![[Pasted image 20240925171006.png]]
Just like with linear regression, we can add higher polynomials of our features and then apply logistic regression. 