### Co-Occurrence mining
Learning a set of items that often show up together. It's a simple idea, but difficult to implement. This is a problem in many categories. Shopping, streaming...

The biggest problem is scale. potentially millions of items and customers

![[Pasted image 20241118164027.png]]
Baskets are also known as *item sets*.

We start by looking at common single items. M, C, B, and J all appear often, but pepsi does not. Therefore, we don't need to consider it for larger sets.

Next, we search for frequent pairs. Once we have our set of frequent pairs, we only need to consider larger sets made of those pairs.

In this example, there are no sets of three or more that show up more than the support threshold.

### Association rules
if-then: ${i_1,i_2,...,i_k}\rightarrow{}j$ means: "if a basket contains all $i_1,...,i_k$, it likely contains $j$."

Confidence: probability of $j$, given $i_1,...,i_k$.

![[Pasted image 20241118164539.png]]
# Apriori Principle
if a set of items $A$ is frequent, then all subsets,  the powerset $\mathcal{P}(A)$, are all frequent.

There's also the contrapositive. If there exists a set $B$ in $\mathcal{P}(A)$ that isn't frequent, than $A$ is not frequent.

![[Pasted image 20241118164917.png]]

so $C_1$ is the set of all individual items.
in the first pass, we count all individual items, and track all items that pass the support count threshold. All frequent single items are put into $F_1$.

$C_2$ is the set of all pairs of items in $F_1$, all the pairs of frequent items. We then count the occurrences of each pair, and all pairs that pass the support threshold are put into $F_2$.

$C_3$ is the set of all possible frequent triples. Each set in $C_3$ is made of two pairs that share one item with the other. This could also be said as two pairs with only one item different. $F_3$ is all of the frequent sets of three items.

$C_4$ continues the pattern, all sets are pairs in $F_3$ that are only one item different, $F_4$ are all frequent groups of four items.

as pseudocode:
```
k <- 0
repeat until no new frequent itemsets are found:
  k <- k+1
  Generate k-length candidates from length k-1 frequent itemsets (make Ck)
  Prune candidate itemsets containing subsets of length k-1 that are infrequent (improve Ck)
  Count the support/frequency of each candidate, keep only those that are frequent (obtain Fk)
```

![[Pasted image 20241118165908.png]]


#### Example:
![[Pasted image 20241118165943.png]]

start by making  $C_1$, the set of all single items.
We then count all with 3 or more occurrences.
$F_1=${Bread, Milk, Diaper, Beer}
$C_2$=
{Bread,Milk},
{Bread,Diaper},
{Beer,Bread},
{Diaper,Milk},
{Beer,Milk},
{Beer,Diaper}

$F_2$=
{Beer,Diapers},
{Bread,Diapers},
{Bread,Milk},
{Diapers,Milk}

To make the next step faster, we sort into lexical order. Otherwise, we might generate a new set multiple times.

We only merge pairs that differ by exactly one.We can prune it even further by only merging sets that match on all but the last items, this will still get every valid combination.
This leads to only one pair

$C_3$=
{Bread,Diapers,Milk}

This one set is not frequent, so we don't go any further.

### Hash trees
Used to reduce the number of comparisons, since this is often used in truly large datasets.

![[Pasted image 20241118172904.png]]
In this, our has function is just mod 3. The first split is on the first number, the second split is on the second number, and the third split would be on the last item.

Note that all item sets are in leaf nodes, not branch nodes. Branches store information used to search the tree.

We also define a "max leaf size". only when the max leaf size is exceeded will we split any node into children based on the hash function. In this example, the max leaf size is 3.
![[Pasted image 20241118173402.png]]

### Compact representation
Often the output is very big, we need a compact way of storing the output. Here, we have 15 transactions, and 30 items
![[Pasted image 20241118173609.png]]
Notice that the majority of transactions don't buy the majority of items, there's large amounts of empty space. In this specific example with this small dataset, we have 3069 frequent sets.

Maximal Frequent Itemsets:
- Only output the maximum frequent itemsets, that aren't in any larger frequent itemsets. 
- Since we know all subsets are also frequent, we don't need to return any of the subsets.
- ![[Pasted image 20241118173931.png]]
- In this example, we would only return the blue itemsets, and we can derive the rest.
- The problem with this method is that we don't keep the frequency of all derived itemsets, we just know that they are frequent.

Closed Frequent Itemsets
- a frequent itemset is closed if none of its immediate supersets have the same support.
- This returns all maximal itemsets, and then some more, but likely not all
- ![[Pasted image 20241118174326.png]]
- In the picture, each single digit represents a transaction, so "124" means the set is in 1, 2, and 4.

In practice, how do we get the closed itemset?
After computing $F_{k+1}$, remove all itemsets in $F_k$ that have a support equal to the support of one of its supersets in $F_{k+1}$. Specifically the superset with the lowest support.

### Problems with Apriori
- Need to scan the dataset multiple times
- Produces many candidate itemsets
- Inefficient for lang patterns
These all lead to a high computational cost, and large I/Overhead on the disk.

# FP-Tree/FP-Growth
represent the dataset as an FP-tree (Frequent-Prefix tree)

Only needs two passes over the dataset:
1. Scan the data to determine the support count of each item. Discard infrequent ones, sort the rest by support count
2. Create the FPtree from the sorted list of high-support items

Example:
step one: get item counts
![[Pasted image 20241120164601.png]]

Now, how do we actually form the tree?

Read each transaction, with each transaction sorted by frequency
![[Pasted image 20241120164738.png]]

The root of the tree is always `null`. Every time we read a transaction, we add that transaction *in order of support frequency*. Every time we traverse through a pre-existing node, we increase its count by one. Eventually, we end up with the final tree:
![[Pasted image 20241120164934.png]]
from the header table, the table that has all the frequent items and their support, we create "chain pointers". These point to every instance of that item. In the image above it only shows the pointers to D and E, but in reality all items have pointers.

To get all paths/transactions containing E, we just look at each E-node in the tree, and traverse bottom-up to the root. Now we can get the "projection tree" for E:
![[Pasted image 20241120165514.png]]
The "Conditional Base" for E is the set of all transactions containing E
{B,C}
{A,C,D}
{A,D

From this, we make a new header table. In this case, B only has a count of 1, so it gets removed along with its branch. In the end, we get this new header table and tree:
![[Pasted image 20241120170114.png]]
We can repeat this with D, to get the paths that lead up to DE:
![[Pasted image 20241120170223.png]]
The left branch of only C was removed, and so was the C that was a child of A. Now that we have only a single path, we've reached the "base of recursion".

We repeat this process with CE, BE, and AE until they've all reached the base of recursion.

What if we went from the start, but with suffix D instead of E? We get the subset of the tree of all branches that contain D, and we remove any children of D nodes.
![[Pasted image 20241120170506.png]]
Then we make a new header table. Note that E won't be in it in this case, but otherwise nothing needs to be removed.
![[Pasted image 20241120170631.png]]
Now just like with E, we continue recursively. We've looked at the  suffix D, but what about AD, BD, and CD?