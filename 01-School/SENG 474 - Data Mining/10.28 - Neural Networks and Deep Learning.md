Take the example of recognizing written numbers, how do we do that algorithmically?

Not easy to do, and might fail if the number is rotated or shifted. Instead, we use neural networks.

four steps for deep learning:
1. Get a large number of training instances
2. use these to automatically infer rules for recognizing patterns
3. create network of "neurons", which each do something similar to logistic regression
4. organize network into layers, outputs of one go into the next.

### Simple example
single neuron, takes in each value x and multiplies it by some coefficient. then, apply some activation function to get the result y.

A slightly larger example could be with multiple neurons:
![[Pasted image 20241028170116.png]]
there's 5 "input nodes" that don't do any math, they just represent the input values. Then, each neuron takes in values from those nodes and multiplies it by some constant. In this example, not every neuron is connected to every other in the previous node, because we know already what features relate to what neurons. This isn't always the case.
![[Pasted image 20241028170339.png]]
This graph is densely populated. Note that any middle layer is called a "hidden layer".

densely populated graphs are not always used in the real world, do to the number of edges required.

How do we decide what to do in each neuron? We can use gradient descent to find the best constant to multiply by.
for binary classification, you only need one output neuron. 
For general classification with $n$ classes, we need $n$ output neurons.


## Handwritten Digits
we have $28*28$ images, so we need $28*28=784$ input neurons. We also need 1 output neuron


### Math
**Cost function**
$$J(w)=\frac{1}{m}\sum_{i=1}^m(y'log(a)log(1-a'))$$
### Non-binary classification
we still have 784 inputs, but now we need 20 output neurons, one for each digit 0-9.
![[Pasted image 20241028172918.png]]
each logit (z), of which in this case there are 10, are calculated with $z_i=w_i*x+b_i$, where $w_i$ is the vector of weights for that neuron.
We used 10, or set $h=10$, because there's 20 outputs.
Now, for the output layer, we make $z_i$ an exponential and use the following equation:
$$a_i=\frac{e^{z_i}}{\sum_{k=0}^he^{z_k}}$$
we do this so the sum of each output adds up to exactly 1.

We then could turn this into a one-hot vector, where the most likely value is set to one, while all the others are set to zero.

##### One-hot
let $y$ equal $7$
we can make the class vector $y=[0,0,0,0,0,0,0,7,0,0,]$
We now use the Log-likelihood loss function, a.k.a. the multi-class cross-entropy function.
![[Pasted image 20241028173541.png]]
### Why use activation functions?
If you only have linear multiplication in each neuron and nothing else, you'll end up with a purely linear model in the end, equivalent to a single neuron, no matter how bit the model is.

You need to add some non-linear elements in order to get a better model. the activation functions are used to accomplish this.

#### Review
$s$ input neurons $x_1,...,x_s$ that only provide input values
some number of hidden layers
$a_1^{(1)},...,a_t^{(1)}$
 $u$ output neurons
$a_1^{(2)},...,a_t^{(2)}$ 

each neuron in the hidden our output layer has a set of weights that it multiplies the input by. You can merge each row of neurons into a matrix
$W^1$ is the matrix of $a^{(1)}$, one column per neuron so it's $s\times{}t$ 

$W^{(2)}: t\times{}u$

each neuron has a bias vector $b$
$b^{(1)}:1\times{}t$
$b^{(2)}:1\times{}u$


##### Forward propagation:
we put in an input vector, what happens?

1. input vector $x$ with dimensions $1\times{}s$
$$z^{(1)}=xW^{(1)}+b^{(1)}$$
now $z^{(1)}$ has dimensions $1\times{}t$.

2. $a^{(1)}$ is obtained by using the activation function on $z^{(1)}$, with the activation function being something such as $\sigma$, Relu, etc
ex: $a^{(1)}=\sigma(z^{(1)})$

3. Calculate $z^{(2)}$
$$z^{(2)}=a^{(1)}W^{(2)}+b^{(2)}$$
now $z^{(2)}$ has dimensions $1\times{}u$.

4. calculate $a^{(2)}=softmax(z^{(2)}$)

softmax is another example activation function. It's often used in the final layer of classification models. Remember, softmax scales all the values so that their sum equals 1.

example: suppose $z^{(2)}=(1,10,100)$
$$s^{(2)}=(\frac{e^1}{e^{1}+e^{10}+e^{100}},\frac{e^{10}}{e^{1}+e^{10}+e^{100}},\frac{e^{100}}{e^{1}+e^{10}+e^{100}})$$
___
When we have several inputs:

instead of just $x$, we have $m$ instances and still $s$ features. We get $X$ with the shape $m\times{}s$.

$$Z^{(1)}=XW^{(1)}+b^{(1)}$$
Note that b is broadcasted $m$ times so it will be $m\times{}t$. This is just so it applies to the whole matrix

$$A^{(1)}=\sigma(Z^{(1)})$$
Remember that $\sigma$ is just an example function, there are others.
$$Z^{(2)}=A^{(1)}W^{(2)}+b^{(2)}$$
$$A^{(2)}=softmax(Z^{(2)},dim=1)$$
what is the dim=1? this is because we want to specifically apply softmax to each row. This might be written as:
```
apply(A_2,
	 dim=1,
	 func=softmax)
```

We can generalize easily:

$$Z^{(l)}=A^{(l-1)}W^{(l)}+b^{(l)}$$
$$A^{(l)}=\sigma(Z^{(l)})$$
$$A^{(L)}=softmax(Z^{(L)})$$
where $l$ is a hidden layer, $L$ is the last layer.

