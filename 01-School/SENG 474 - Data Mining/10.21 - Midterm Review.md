### Data Prep
Transforms: convert data into more convenient shape
- log transform: good to remove skew
Encoders: convert non-numeric values into numeric values
- simple: replace each values with 1,2,3,...
- one Hot: each possible value gets a column, 0 in all columns not used in that sample
Scalers: scale all values into one standardized interval so one column isn't weighted too much
- min max: scales evenly from 0-1
- standard: more resistant to outliers
Pipeline: multiple preparation steps to be done all at once
### Evaluating a model
confusion matrix:

|            | Predicted:true  | Predicted:false |
| ---------- | --------------- | --------------- |
| Real:true  | true positives  | false negatives |
| Real:false | false positives | true negatives  |
True positive rate:
$$\frac{TP}{TP+FN}$$
False positive rate:
$$\frac{FP}{TN+FP}$$
Precision:
$$\frac{TP}{TP+FP}$$
Recall = true positive rate:
$$\frac{TP}{TP+FN}$$
F measure:
$$\frac{2}{\frac{1}{precision}+\frac{1}{recall}}=\frac{2(precision)(recall)}{precision + recall}$$

the classifier gives a score on how confident the value is true. The bigger the score, the more confident it's true. We can change this threshold to maximise precision and recall.

Sensitivity: same as recall and TPR
$$\frac{TP}{P}$$
Specificity: important in medical field
$$\frac{TN}{TN+FP}$$
## Algorithms
### Linear Regression
hypothesis: $h\theta(x)=x*\theta$
- what the model expects $x$ to be
cost: $J(\theta)=\frac{1}{2m}sum((h_\theta(X)-y)^2)$
- the total error
Gradient: $G(\theta)=\frac{1}{m}X^T(h_\theta(X)-y)$
- the slope of the cost, used for gradient descent

repeat some number of times, alpha is how much to move each iteration
`theta -= alpha * gradient(theta)`

### Logistic Regression
cost is more complex, but same idea as linear regression.
use gradient descent to find hyperplane of best for for dataset, separating yes\no values. 
### Support Vector Machines (SVM)
checks dot product between points and separator line to fine a line to separate the yes\no values. We want the margin from the line to the closest points to be as large as possible.
Closest point to line are support vectors.

distance to closest points, or the margin, is:
$$\frac{1}{||w||}$$
Our goal is to maximise $||w||$ to minimize the margin.

##### Soft margin
we want to ignore outliers as they can lead to very small margins.
$$min_{w,b,\xi}\frac{1}{2}w*{}w+C\sum_{k=1}^m\xi_k$$
$C$ is a hyperparameter on how much slack you want, too big means it overfit and too small means underfitting.

$\xi_k$ is how far into the margin a point is

for well-classified points, $\xi_k=0$ 
for points in the margin, $\xi_k>0$
	if it's on the right side, it's less than one
	if it's right on the line, it equals one
	if it's on the wrong side, it's between one and two
for points of the margin, support vectors, $\xi_k=1$.
### Decision trees
split dataset based off of attribute that gives the least entropy.
$$entropy(P_1,...,p_n)=p_1log(p_1)+...+p_nlog(p_n)$$you could also use GINI
$$GINI(p_1,...,p_n)=1-\sum_{i=1}^np_i^2$$
### Naive Bayes
all about probabilities
