linear SVMs are a type of classifier that predicts a new instance of $x$ by first computing the decision function:
$$\theta{}x=\theta_0x_0+...+\theta_nx_n$$
This formula is just like logical regression, $x_0$ is the bias feature, it always equals 1

the conventional formula doesn't use $\theta_0x_0$, instead it takes the form: $$wx+b=w_1x_1+...+w_nx_n+b$$
if the result is positive, then the predicted $\hat{y}$ is the positive class (1), otherwise it's the negative class (0). This is the type of classifier that returns true\false values
this whole equation can be written as:

$$h(x)=sign(wx+b)=\pm(1)$$

but how do we choose what line $w+b$ to use?
we one to choose the line equidistant from "each class". Each point in the dataset will be either 1 or -1, we want to choose a line that's equally distant from both classes.
![[Pasted image 20241002164312.png]]
As shown in this image, the bottom line is much further from either class, so it's the better line to choose.

The closest points, the ones on the dotted lines are called the "support vectors"


the distance of a point to the line
$$\frac{|w*x^k+b|}{||w||}$$

the distances to the closest points $k_1,k_2$, is calculated with:
$$\frac{|w*x^{k_1}+b|}{||w||}=\frac{|w*x^{k_2}+b|}{||w||}=\frac{\gamma}{||w||}$$
for both lines, gamma is the same. This is because we want the line to be equidistant from both points.
this value, $\gamma/||w||$ is called the margin. We want to maximise this.


we want to rescale w and b such that
$w*x^{k_1}+b=1$ for the closest points on the +1 side, and such that
$w*x^{k_2}+b=-1$$ on the -1 side

we want a line that maximises the margin $1/||w||$, which is the same as minimising $||w||$. With this, we've rephrased the problem as an optimisation problem

we want to minimize $\frac{1}{2}w*w$, constrained by $y^k(w*x^k+b)\geq1$, and for the closest points we want it to equal exactly 1.

We want to find the best values for $w$ and for $b$, called $w^*,b^*$.

### Soft Margin
There may be some outliers, we want the *soft* margin to give us some slack to ignore these outliers.
![[Pasted image 20241002170748.png]]
That one red dot is an outlier, if we use it as a support vector then the boundary line will be moved significantly.
We modify the above equations slightly.
Now, we want:
$$min_{w,b,\xi}\frac{1}{2}w*w+C\sum_{k=1}^m\xi_k$$
and it is constrained by:
$$y^k(w*x^k+b)\geq1-\xi_k, k\in[1,m], \xi_k\geq0$$

the squiggle is the symbol xi
C is a hyperparameter which controls how much slack we're given. Bigger Cs mean more slack, potentially overfitting the data. Smaller Cs mean more slack, potentially underfitting.

### Types of points
There's three types of points:
##### Well Classified
This is where the point is outside of the margin, there's no need for slack so $\xi_k^*=0$.
$y^k(w*x^k+b*)\gt1$
##### Margin Errors
This is where the point is in the margin, so we do need some slack $\xi_k^*\gt0$
$y^k(w*x^k+b^*)\geq1$
##### On the Margin
these lines are directly on the margin, so $\xi_k^*=0$ and  $y^k(w*x^k+b*)=1$

### Kernel trick
what do we do if our data is like this?
![[Pasted image 20241002173418.png]]

we create new attributes out of existing ones, there's a few ways to do so
- add polynomials based on existing features (polynomial kernel)
- the preferred way is to add new features based on the closeness of data point from some landmark points
Using landmark points is what we'll cover.

![[Pasted image 20241002173553.png]]

where the $\ell$ (\\ell) represents each landmark point

![[Pasted image 20241002173638.png]]

#### How do we choose landmarks?
all of the training data points become landmarks. Then, compute $f_1,...,f_m$ for each training point.
Finally, find the hyperplane maximum margin classifier using these new features.

The problem with adding more features is overfitting, we need to add the hyperparameter $\gamma$. High gamma increases overfitting, limits influence to closer points.
Low gamma influences further points, creating smoother edges, possibly underfitting
You can find a good gamma with cross-validation
![[Pasted image 20241002174323.png]]