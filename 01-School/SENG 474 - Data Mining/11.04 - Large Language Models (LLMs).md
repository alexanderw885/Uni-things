Using [Word2Vec](10.30%20-%20Word2Vec), we can store words as vectors while retaining their meaning and relation to other words. How can we store words in the vector space while keeping all of its different meanings?
We need to use context to determine the meaning of a word.
### Attention Mechanism
![[Pasted image 20241104164212.png]]
To move a word closer to another, you would average their values. Moving word "one" closer to "two" by 10% would  be done by:
$$avg = 0.9(one)+0.1(two)$$
### Transformers
Maintains context to produce a coherent text.

It does this by, once it selects a starting word, taking the already predicted words and then finding the most likely words to follow it. The model generates the response one word at a time.

In order to ensure that every sentence has a unique vector, we use positional encoding. This ensures that sentences like "I'm happy, not sad" is unique from "I'm sad, not happy"

Once we've finished with the transformer, we use the softmax layer at the end to convert the output into probability scores.

