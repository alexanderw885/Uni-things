Finding groups of similar objects, separate from other groups of objects.

Good for a number of topics, such as categorizing documents by topic, or grouping customers for targeted analysis. It can even be used to group stocks by similar price fluctuations.

You can aggregate the data from a group all together, and then run analysis of the aggregated data.
### Similarity and Dissimilarity
Both numerical measures of how alike or different two data objects are.

When all attributes are continuous, we can use euclidean distance, though scaling may be required if attributes have different scales. To compare two objects $p,q$, each with $n$ attributes:
$$dist=\sqrt{\sum_{k=1}^n(p_k-q_k)^2}$$

We can generalize this to "Minkowski Distance"
$$dist=^r\sqrt{\sum_{k=1}^n|p_k-q_k|^r}$$
Where r is a parameter. What does it do?

$r=1$ gets city block distance, only able to move in orthogonal directions, one vector at a time. Good for clustering or frequency data, works well in many dimensions

$r=2$ is euclidean, good for continuous spaces

$r>2$ "supremum" distance, good for when extreme values are very important

as $r$ approaches infinity, only cares about maximum deviation, good for worst-case scenarios.

### Algorithms
#### K-mean clustering
Each cluster gets a centroid. group each point with the closest centroid. Move the centroid to the centre of this new cluster. Repeat until convergence.

The initial centroids may be chosen randomly, though different initial positions may lead to different results. Good practice is to run the algorithm multiple times and choose the result with the least squared standard error SSE
$$SSE=\sum_{i=1}^k\sum_{x\in{}C_i}[dist(m_i,x)]^2$$
$x$ is a datapoint in the cluster $C_i$
$m_i$ is the centroid of cluster $C_i$
All this is doing is adding up the distances from each point to their centroid. the square is both to remove negative values and to place more emphasis on outliers.

How can we reduce SSE?
An easy way is to use a larger $K$, more centroids. SSE decreases as we increase $K$, though be careful of overfitting. The SSE will fall rapidly as $K$ decreases, and then very slowly after the ideal value for $K$. Once this decrease slows down, stop.

This method, K-means, has some problems:
- trouble with differing sizes of clusters
- trouble with different cluster densities
- trouble with "non-globular" shapes

One strategy to get around these difficulties is to start with a large number of clusters, and then merge them together once the algorithm is finished. Merge the clusters that would increase SSE the least.

Another strategy is to choose good initial means, though this can be very difficult to do in large datasets with many true clusters or many dimensions.
#### Bisecting K-means
extension of K-means clustering: Split the set into 2 clusters, then select one of these clusters to split. repeat until you have $K$ clusters. You can do this by performing "trial" bisections, and selecting the one with the least SSE
#### Hierarchical Clustering
Similar to algorithms in [[Spatial Data Structures]], CSC 305. Create a set of *nested* clusters, that are organized into a tree

Hierarchical is useful in that you don't have to assume/guess any number of clusters. It also has real-world examples, such as taxonomy.

However, it's computation and memory intensive.

method:
```
let each point be a cluster
while there's more than one cluster:
    merge the two closest clusters
```

How do we define similarity or dissimilarity?
- min distance: distance between closest points
	- Can handle "non-globular" shapes
	- Very sensitive to outliers
- max distance: distance between farthest points
	- not sensitive to outliers
	- often fails when dealing with clusters of different sizes
- group average: average the distances of every point in one group to every point in the other.
	- $O(N^2)$ space
	- often $O(N^3)$ time, in some cases can be brought down to $O(N^2log(N))$.

These can all be accomplished by pre-computing a matrix of the distances between every pair of points, though this is memory-intensive.

#### DBScan
Locates regions of high density separated by areas of low density

Density is the number of points within a specified radius Eps. We can classify points into 3 groups:
- Core points: more than a specified number of points (MinPts) within this radius Eps
- Border point: fewer than MinPts in radius Eps, but within the range of a core point
- Noise point: not a core or border point. We can ignore the noise points.

algorithm:
- if any core points are within Eps of each other, put them in the same cluster
- any border points go into the same cluster as their nearest core point
- noise points are not considered.

It's resistant to noise, and it can adjust to different sized/shaped clusters
It can be hard to run on datasets with clusters of multiple different densities, especially if there's somewhat dense noise and some sparse clusters.

You need to decide Eps ($\epsilon$) and MinPts. It can take some trial and error, or testing on benchmarks with already known clusters.

How do we choose our hyperparameters?
1. compute distances from each point p to it's k-th nearest distance
	- Cluster points will have a small k-dist, noise will have high k-dist
2. sort all k-dist values into a plot. The point where the k-dist sharply increases is a good Eps value
3. MinPts should be set to $k$