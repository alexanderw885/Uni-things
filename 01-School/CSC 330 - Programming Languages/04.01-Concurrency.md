General definition: Separate and distinct computations that *may* be performed at the same time. They don't have to be, this refers to the relation between computations.
it also might mean:
- the behaviour of threads or processes all running on one processor
- behaviour of threads on processes on multiple cores
- programs that can carry out multiple operations at once
- multiple executions in parallel on a CPU

Concurrency is very important for computer systems. For example, device drivers have to communicate between potentially very slow external devices and a very fast CPU. In order to facilitate this, we need concurrency. 
It's also needed for software-as-a-service architecture, and for any remote procedure calls.

**Difference between concurrency and parallelism**
Concurrency does not require multiple processing units, though it can use them
Parallelism requires the means to compute multiple things at the same time. Parallelism *uses* concurrency to decrease runtime.

Concurrency can add non-deterministic aspects to a process, and make everything much harder. You need to worry about race conditions, and ensure there's no deadlocks.

### Where do programming languages fit?
1. Expressing sequential computation
2. Creating threads (extending `Thread` class in Java, using `pthread` in C, or creating new processes)
	- Note that there's often specific ways that different types of threads can communicate
	- Threads can be heavyweight (separate processes), or lightweight.
3. Controlling thread interactions
	- help manage non-determinism
	- allows the programmer to "prune" less desirable non-determinism with tools like `pthread_mutex` in C, or `synchronize` in Java. They're locks, so only one thread can "acquire" some resource.
	- Condition variables, or `convars` allow us to only let a thread access some section unless a condition is met.