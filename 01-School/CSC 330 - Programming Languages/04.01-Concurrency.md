General definition: Separate and distinct computations that *may* be performed at the same time. They don't have to be, this refers to the relation between computations.
it also might mean:
- the behaviour of threads or processes all running on one processor
- behaviour of threads on processes on multiple cores
- programs that can carry out multiple operations at once
- multiple executions in parallel on a CPU

Concurrency is very important for computer systems. For example, device drivers have to communicate between potentially very slow external devices and a very fast CPU. In order to facilitate this, we need concurrency. 
It's also needed for software-as-a-service architecture, and for any remote procedure calls.

**Difference between concurrency and parallelism**
Concurrency does not require multiple processing units, though it can use them
Parallelism requires the means to compute multiple things at the same time. Parallelism *uses* concurrency to decrease runtime.

Concurrency can add non-deterministic aspects to a process, and make everything much harder. You need to worry about race conditions, and ensure there's no deadlocks.

### Where do programming languages fit?
1. Expressing sequential computation
2. Creating threads (extending `Thread` class in Java, using `pthread` in C, or creating new processes)
	- Note that there's often specific ways that different types of threads can communicate
	- Threads can be heavyweight (separate processes), or lightweight.
3. Controlling thread interactions
	- help manage non-determinism
	- allows the programmer to "prune" less desirable non-determinism with tools like `pthread_mutex` in C, or `synchronize` in Java. They're locks, so only one thread can "acquire" some resource.
	- Condition variables, or `convars` allow us to only let a thread access some section unless a condition is met.

### Threads in Haskell
uses `forkIO` operation. Note that the original thread continues to run, alongside the newly created thread.

How do we share data between these threads? 
we use the datatype `MVar`, which has 4 operations:
- `newEmptyMVar`
- `newMVar` creates with specific val
- `takeMVar` if the MVar is empty, suspend the thread: otherwise, take value out of MVar and return it to calling thread
- `putMVar` put value into MVar. If it's full, suspend the thread.

### Concurrency in Rust
- code executed in a thread is contained in a closure
- ownership of shared resources like mutexes does matter
- smart pointers like with `Box` are needed

create threads with `std::thread::spawn`
Like C, threads are terminated when the main thread finishes.
To wait for a thread to complete, use
`handle.join().unwrap`
The unwrap is to ignore any returned variables.
```rust
fn main() {  
thread::spawn(|| {  
  for i in 1..10 {  
    println!("hi number {i} from the spawned thread!"); 
    thread::sleep(Duration::from_millis(1));  
}  
});
```
the `|| {function}` is a closure.

How do we share data? If data is created in the main thread, you need to use the `move` keyword. This moves data to be owned by the spawned thread
```rust
fn main() {
  let v = vec![1,2,3];

  let handle = thread::spawn(move || {
    println!("Vector: {:?}", v);
  });
  handle.join().unwrap();
}
```
Notice that we didn't specify to move `v`, the compiler did this for us.
We can also use multiple producer, single consumer with `std::sync::mpsc`
make sure to add `move` to every closure using this!
`let (tx, rx) = mpsc::channel()`
use `tx.send(value)` to produce values, and you can iterate through `rx` with a for loop