#### Terms
- Syntax: form or structure of the expressions, statements, and program units in a language
- Semantics: meaning of the expressions, statements, and program units
Together, the syntax and semantics form the definition of a programming language. This definition used by both language designers and implementers. Sometimes programmers using the language might use the definition to help them understand the language.

- Sentence: string of characters, over some alphabet (a full computer program)
- Language: a set of legal sentences
- Lexeme: the lowest level syntactic unit of a language (keywords, `sum, with, *, +`)
- Token: category of lexemes. Could be a lexeme, could be a variable,... (not necessarily unique)
- Language recognizers: read input strings in the alphabet of a language, decides if string belongs to the language (syntax analysis in a compiler)
- Language generators: generates valid sentences for a language. More theoretical than a recognizer.
### Backus-Naur Form (BNF)
Invented by John Backus, describes syntax of Algol 58.  BNF is equivalent to a context-free grammar.

in BNF, abstractions are used to represent classes of syntactic structures. Each abstraction acts like syntactic variables (aka nonterminal symbols or terminals.)
Terminals are lexemes or tokens
Rule structure: every rule has a left-hand side, and a right hand side
- left hand side LHS is a nonterminal
- right hand side RHS is a string of terminals, string of nonterminals, or both

Nonterminals are often in angle brackets.
Example:
`<ident_list> <- identifier | identifier <ident_list>`
`<if_stmt> -> if <logic_expr> then <stmt>

a grammar is finite, non-empty set of rules.

Each abstraction can have more than one RHS
`<stmt> >- <single_stmt>`
	 `  |  <stmt_list> end`

recursion is allowed
`<ident_list> -> ident`
			`| ident, <ident_list>`

a derivation is a repeated application of rules, until you get only terminal symbols, it cannot be broken down any further.
![[Pasted image 20250115130038.png]]

A program can be broken down into statements
statements can be broken into a single statement, or into one statement and multiple statements. Essentially, can turn into an arbitrary number of single statements.
Each single statement is `<var>=<expr>`
there's four variables
each expression is either the addition or subtraction of two terms 
each term is a variable of constant

This exactly what we did to prove a string is in a context-free grammar in CSC 320
![[Pasted image 20250115130426.png]]


### Derivations
Each string of symbols in a derivation is a sentential form
a sentence is a sentential form that only has terminal symbols.
A leftmost derivation is when you always expand the leftmost nonterminal. This is not always required, it's just a convention.

Note that derivations can be shown in a parse tree:
![[Pasted image 20250115131642.png]]

If a grammar lets you generate a sentential form with two or more different parse trees, it's ambiguous. This is generally not good, and extra rules have to be provided to the compiler to prevent this.
![[Pasted image 20250115131848.png]]
In this case both trees lead to the same result but arrived there in different ways. In the left example, the subtraction expression appears as though it should be performed first, while in the right tree the \* looks like it should be done first.

In order to get around this ambiguity, we add a rule:
- The parse tree indicates the precedence of operators

#### Extended BNF (EBNF)
optional parts are placed in brackets `[]`
`<proc_call> -> ident [(expr_list)]`

alternative are in parenthesis, with a vertical bar for OR
`<term> -> <term> (+|-) const`

repetitions in curly brackets. These can be 0 or more
`<ident> -> letter {letter | digit}`

___

CFGs cannot explain all the syntax of programming languages, so we use:
### Attribute Grammars (AG)
AGs are an "enriched grammar notation" that adds semantic notation to a CFG.
We "decorate" the nodes in a parse tree with additional attributes, and constraints on how these attributes relate to each other

an AG is a CFG G=(S,N,T,P) with the following additions:
- for each grammar symbol $x$, there is a set $A(x)$ of attribute variables
- each rule has a set of functions that define certain attributes of the non-terminals in the rule
- each rule has a set of predicates to check for consistency (can be empty set)

![[Pasted image 20250117125602.png]]
![[Pasted image 20250117125620.png]]
`actual_type` and `expected_type` are attributed added by the AG
![[Pasted image 20250117125746.png]]
the `[1]` is not indexing, it's like a subscript \<var>$_1$.
These rules together are ensuring that the two variables being added together are of the same type.

___
### Semantics
It's possible to make a sentence in English that's syntactically valid, but makes no sense. We can do the same in programming languages. There's no single way that was widely accepted to formalise semantics.

Operational Semantics: describes meaning of program by executing statements on simulated or actual machine. To use operational semantics on a high-level language, some virtual machine must already be defined.

Denotational Semantics: Based on recursive function theory, most abstract form of semantic descriptions.
1. Define mathematical object for each language entity
2. Define a function that maps instances of those entities onto their mathematical objects
This can be used to prove correctness of semantics, and gives a rigorous way to think about programs.

In operational semantics, state changes are defined by coded algorithms.
In Denotational semantics, they're defined by rigorous mathematical statements



