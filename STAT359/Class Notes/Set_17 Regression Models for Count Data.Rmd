---
title: "Set_17"
author: "Alexander Williams"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
```

# Regression Models for Count Data

So far we've used regression models for data with normal distributions. Now, what if we're looking at counting data, that takes values like 1,2,3,...? Ex: number of events in a fixed period of time

### Example: Tree infections in White Spruce Plantations

```{r}
weevil<-read.table(file='~/Documents/uni/STAT359/data/weevil_stat359.txt',
                   header=TRUE,sep="")
nrow(weevil)
kable(weevil[1:25,], caption = 'White Spruce Infection Data')
```
Tree id, x/y coordinates, height of the tree, and number of infections.

The response is the number of infections on each tree.

```{r}
table(weevil$N10)
```
2655 trees never infected, 1158 infected once, and so on
```{r}
hist(weevil$N10, main = 'Number of Infections - 10 Years',xlab = 'Infections')
```

This is obviously not normal, what does height look like?

```{r}
hist(weevil$H10, main = 'Height at 10 Years')
```

What about tree locations?
```{r}
plot(weevil$x,weevil$y,cex=0.5)
title('Tree Locations')
```

what happens if we try linear regression? It won't really work since the output would be continuous, while we're expecting a discrete count. Same issue with a log transform, we'll still be trying to predict a discrete value. What if we do ti anyways?

```{r}
par(mfrow=c(2,1))
fit1<-lm(log(N10+0.0005)~ x + y + H10,data=weevil)
plot(fit1,which=c(1,2))
```

That looks really bad, we could not use it.

Since this is using counts, this is the type of data that might come from a $Poisson(\mu)$ distribution. How can we use this instead?

Lets assume the distribution is Poisson.
$$Y_i\sim{}Poisson(\mu_i)$$
$$log(\mu_i)=\beta_0+\beta_1x_{1i}+...+\beta_px_{pi}$$
Why do we take the logarithm? it's so that the target isn't constrained to just positive values, now it can be negative. This makes it more natural to model.

Just like with linear regression, we want to predict the parameters $\beta$. The only difference is we don't add $\epsilon\sim{}N(0,\sigma^2)$, since this isn't normal. 

How do we interpret each $\beta$ value? it's a bit more complicated than with linear regression due to the log transform on $\mu$.
Lets say we change $x_1$ to $x_1+1$. the new $\mu$ is:
$$\mu_{new}=e^{\beta_i}\mu_{old}$$
Changing any $x_i$ multiplies the old $\mu$ by the exponential of $\beta_i$ multiplied by the change in $x_i$.


Now, how do we estimate $\beta$? We use maximum likelihood estimators. What is the likelihood function?
$$L(\beta)=f(y_1,...,y_n;\beta)=\Pi_{i=1}^nf(y_i;\beta)$$
where:
$$f(y_i;\beta)=\frac{exp\{-\mu_i(\beta)\}\mu_i\beta^{x_i}}{y_i!}$$
this is estimated with the `glm()` function. It works very well when you have a large sample size.

Lets show this all works by running the function on a set of data we know all the values or $\beta$s for.

```{r}
# Let's simulate a dataset from a Poisson regression model. 

# We will then fit the Poisson regression to the simulated data and try to recover the parameter values using maximum likelihood.

# The point is to give a demonstration showing that the MLE method can recover the regression coefficients reasonably well from simulated data when the sample size is large.

## Our model will have two covariates and an intercept

# Setting the "true values" of the model parameters
beta0<-2 # intercept
beta1<--2.5 
beta2<-0 # this means x_2 has no influence on Y

# Simulate data
n<-1000 # sample size
# Two covariates in the regression model
X1<-seq(1,100,length=n)/100
X2<-rnorm(n=1000)

# According to the model the log-mean of Yi is then beta0+beta1*X1 + beta2*X2. This is earlier in these notes.

mu.log<-beta0+beta1*X1 + beta2*X2
mu<-exp(mu.log)

# Now simulate the Poisson counts having mean mu
Y<-rpois(n=n,lambda=mu)

## We now have a dataset where we know the true values of the parameters.
hist(Y)
plot(X1,Y)
plot(X2,Y)
```

Visually, it appears there could be a relationship between Y and X1, though the discrete data does make it look strange. It looks less like there's a relationship between Y and X2, matches how we made this data.

Now that we have Y, X1, and X2, we can fit the Poisson regression with `glm()`.
```{r}
fit1<-glm(Y~X1+X2,family=poisson) #  can choose a number of different distributions. We will also use binomial
summary(fit1)
```
First, lets look at the estimates. 

True X0 is 2, estimate is very close

True X1 is -2.5, also very close

True X2 is 0, the model get quite close to this as well.

What about the p-values? The null hypothesis is that that coefficient is zero.

there's strong evidence that X0 and X1 are non-zero, and there's no evidence that X2 is non-zero. this all lines up with the true values we used earlier.

There's also the residual deviance, we can use to check the validity of the model. We want it to be close as possible to the degrees of freedom.
```{r}
deviance(fit1)
fit1$df.residual
```
It's fairly close. We can use this to get the likelihood statistic. the null hypothesis is that the model is adequate, and the alternative is that the model is not adequate. it's a $\chi^2$ distribution.
```{r}
1-pchisq(deviance(fit1), fit1$df.residual)
```
The p-value is very small, which suggests that the model did not fit the data very well.

What if we specifically want to test is one of the $\beta$ values is zero? We can remove $x_i$ from the model and see if the residual deviance changes much. residual deviance always increases when you remove one of the variables.

We do this by testing the difference in residual deviation between the two models under a $\chi^2$ distribution.

Lets test a model without X2, which we know has no impact on Y, and compare to our first model.

In this anova test, our null hypothesis is that the removes parameter was equal to zero.

```{r}
# remove X2 and fit the smaller model
fit2<-glm(Y~X1,family=poisson)
## assess the significance of X2 using a deviance test
anova(fit2,fit1,test="Chi")
```

Based on this likelihood ration test, we would not reject the null hypothesis that $\beta_2=0$.

We can also compute it by hand.

```{r}
1-pchisq(deviance(fit2)-deviance(fit1),df=1)
```


Back to the example, let's fit the tree infection data
```{r}
fit<-glm(N10~ x + y + H10,family=poisson,data=weevil)
summary(fit)
```
This model is saying that x, y, and height all relate to Y. Lets compare residual deviance to degrees of freedom. 

```{r}
1-pchisq(deviance(fit), fit$df.residual)
```
4969 to 4326, the deviance is quite a bit higher, and this is reflected in the very low p-value. This says there's strong evidence against the null hypothesis, evidence that this model is not a good fit. Why could this be?

We're missing a lot of information, since we only have location and height. What about light, or elevation for example?  Not having these leads to what's called "overdispersion", where the variance is greater than the mean: $Var[Y] \gt{} E[Y]$. To use the Poisson distribution, we assume that $Var[Y] = E[Y]$.

with overdispersion, the Poisson model we're using might underestimate the standard errors. We can handle this by adding an extra parameter $\phi$, so $Var[Y]=\phi{}E[Y]$.

```{r}
fit.OD <- glm(N10~x+y+H10,
              family=quasipoisson, # regression model with the additional parameter, still poisson
              data=weevil)
summary(fit.OD)
```

We can see that in this quasi-Poisson model, the p-value for x is an order of magnitude greater. It's still significant, everything still is, but less so. This might not always be the case, sometimes variables will no longer be significant in this model.

the dispersion parameter $\hat\phi=1.25$ means we're stretching out our confidence interval by $\sqrt{1.25}$. 

### Example: Epilepsy study

impact of new treatment on epilepsy, one group receives placebo, the other the treatment. 

response is the number of seizures during the month after randomization.

```{r}
epl<-read.table(file ='~/Documents/uni/STAT359/data/epl.txt',
                header=TRUE,sep="")
epl$sex<-epl$gender ## the label for this variable is not correct. Change to 'sex'. 
epl<-subset(epl,select = -c(gender)) ## drop the old variable from the dataframe
attach(epl)
library(knitr)
nrow(epl)
kable(epl[1:25,], caption = 'Data from Epilepsy Study',align='l')

```

Baseline is the number of seizures the month before randomization. This is a very important variable, including this would likely lead to overdispersion. sex=1 means male, 0 means female. treatment=0 means placebo.

Lets look at the data a little bit.

```{r}
table(epl$Y)
hist(epl$Y,main='Number of seizures',xlab='seizures One Month After Randomization')
boxplot(epl$Y[epl$treatment==1],
        epl$Y[epl$treatment==0],
        names=c("Treated","Not Treated"),main='seizures by Treatment')
```

It does look like a good candidate for a Poisson distribution, going by the histogram

Visually there appears to be more seizures in the group without treatment, along with higher variance. What about grouping by sex?

```{r}
boxplot(epl$Y[epl$sex == 1],
        epl$Y[epl$sex == 0],
        names=c("Male","Female"),main='seizures by sex')

```

This is looks less significant than treated/not-treated, but still potentially significant. Can we see a relation between baseline and seizures after treatment?

```{r}
plot(epl$baseline,
     log(epl$Y+1),
     xlab='Baseline seizures',
     ylab='seizures Month After Randomization',
     main='Y versus Baseline seizures')
```

There appears to be something of a positive relation, as we would expect. Let's fit a model to it. Lets also consider all possible relations between variables

```{r}
fit1<-glm(Y~treatment*sex*age*baseline,family=poisson,data=epl)
summary(fit1)
```
We have so many terms that none appear significant, lets remove one at a time

```{r}
fit2<-update(fit1,.~.-treatment:sex:age:baseline)
summary(fit2)
```
we removed the 4th order interaction, now the 3rd order interactions.
```{r}
fit3<-update(fit2,.~.-treatment:age:baseline)
summary(fit3)
fit4<-update(fit3,.~.-sex:age:baseline)
summary(fit4)
fit5<-update(fit4,.~.-treatment:sex:age)
summary(fit5)
fit6<-update(fit5,.~.-treatment:sex:baseline)
summary(fit6)
```

Now it appears the treatment:sex is very significant, lets continue removing terms. Now, we're at 2nd order interactions.

This is easy to automate, but also it can be good to do manually, to give a better feel for the data

```{r}
fit7<-update(fit6,.~.-age:sex)
summary(fit7)
fit8<-update(fit7,.~.-age:baseline)
summary(fit8)
fit9<-update(fit8,.~.-sex:baseline)
summary(fit9)
fit10<-update(fit9,.~.-treatment:age)
summary(fit10)
fit11<-update(fit10,.~.-treatment:baseline)
summary(fit11)
```

Now everything is significant except for sex, but we don't want to remove it while keeping treatment:sex. Otherwise, everything is significant. Lets now see how well this model fits the data.

```{r}
plot(fit11,which=c(1,2,4))
```

In the residual vs. fitted graph, the variance doesn't diverge from the mean, that's a good sign. Do be careful though, since the data is discrete, the residuals are discrete. That's why we appear to get so many straight lines in this graph.

The residuals appear to be normal, also good. It does appear to bow a little bit, implying some skew, but it's close enough. point 138 does appear to stand out.

In the Cooks' distance graph, point 138 dos appear to have a large influence, though other points also have a large influence. It may be worth removing to see the impact, but it's not enough to be vital.

In this final model, we find that all factors relate to Y, including an interaction between treatment and sex. Both men and women react to treatment, but there's a greater impact on men. Age has a positive impact, so increased age leads to more seizures on average. Same with sex, being male increases the level of seizures.

What's our confidence interval on these values?

```{r}
confint(fit11)
```


