---
title: "Set_18 Binomial Regression"
author: "Alexander Williams"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Binomial Regression

This is good for when the response value is either a 0 or 1, true or false. We want to find how our response might relate to some variables.

The response is discrete, $Y\in\{0,1,...,n\}$, so there's $n$ independent trials. We can model the response as $Y_i\sim{}Bin(n_i,\pi_i)$, where $n_i$ is number of trials and $\pi_i\in(0,1)$. is the chance of success

we can estimate the probability of success as $\pi_i/n_i$, so we estimate $n$ parameters, the unknown true values of each $\pi_i$.

A better way is to estimate all of the $\pi_i$ values at once with a logistic regression model with only $p$ regression parameters and covariates, with $p<n$.



$$log(\frac{\pi_i}{1-\pi_i})=\beta_0+\beta_1x_{i,1}+...+\beta_{p-1}x_{i,p-1}$$

One objective is to then examine how much of an impact using the reduced model to fit the data has compared to the so-called saturated model, and then to determine if $p$ parameters is enough.

Consider the regression model above
$$log\{\frac{\pi_i}{1-\pi_i}\}=\beta_0+\beta_1x_{i,1}+...+\beta_{p-1}x_{i,p-1}$$
we get log-likelihood function:
$$l(\beta_0,...,\beta_{p-1})=\sum_i\{y_ix_i'\beta-m_ilog\{1+e^{x_i'\beta}\}\}$$

We can then get a function to estimate $\pi_i$.

We want to estimate $E[Y_i]=m_i\pi_i$ under saturated and reduced models. In the saturated model we fit the data perfectly $E[Y_i]=y_i$, and with the reduced model we want for regression, we get $\hat{E}[Y_i]=\hat\mu_i=m_i\hat\pi_i$

We want to measure how far apart these are.

in R, we can look at the residual deviance from the `glm` function. As a rule of thumb, it should be less than or on the order of magnitude of the degrees of freedom, $n-p$.

We can use that to find a p-value in a $\chi^2_{n-p}$ distribution, where the null hypothesis is that the reduced model is adequate. 

for comparing two groups, logistic regression takes the form:
$$y_i\sim{}Binomial(m_i,\pi_i)$$
$$log\{\frac{\pi_i}{1-\pi_i}\}=\beta_0+\beta_1x_{1i}$$
where $x_{i1}$ is 1 if subject $i$ is in group 2, and 0 if subject $i$ is in group 1.

when $x_{1i}=0$, the intercept, $\beta_0$ is the log-odds of event in group 2: $log\{\frac{\pi_2'}{1-\pi_2'}\}$.

when $x_{1i}=1$,  $\beta_0+\beta_1$ is the log-odds of event in group 1: $log\{\frac{\pi_1'}{1-\pi_1'}\}$. 

The betas in logistic regression can be interpreted as the log-odds ratio of probabilities being in one of the two groups.

## Not-code example
suppose $Y_i\sim{}Bin(n_i=1,\pi_i)$.
$Y_{i1}$ = 1 of subject had heart attack, 0 if they did not 

$x_{i1}$: 1 means smoker, 0 means non-smoker

$x_{i2}$: 1 if heavy drinker, 0 if light drinker.

$x_{i3}$: smoker and light drinker

if a subject doesn't smoke or drink, their odds are $e^{\beta_0}$, since all of the other betas are cancelled out.

If they don't smoke but are a heavy drinker: $e^{\beta_0+\beta_2}$, since now $x_{2i}$ is 1.

In general, we know that the odds $\frac{\pi_i}{1-\pi_i}=e^{x_i'\beta}$.

now that we have this, we can find things like "What are the relative odds comparing heavy drinkers to non-heavy drinkers?
$$\frac{exp(\beta_0+\beta_2)}{exp(\beta_0)}=exp(\beta_2)$$
each $beta$ value is the odds of that variable over the intercept/baseline, smoker/non-smoking, or drinking/non-drinking.

what if we add in $\beta_3x_{i3}$, with the interaction between smoking and drinking. Now, drinking impacts the odds of heart attack while smoking, P(heart attack + smoking | drinking) != P(heart attack + smoking | !drinking). Before adding the interaction, these would have been the same.

## Example
Estimation of Prognosis for Children with Neuroblastoma

Goal is to investigate relationship between probability of surviving 2 years free of disease following diagnosis and treatment, and stage of disease at diagnosis

```{r}
neuro.dat<-read.table(file='~/Documents/uni/STAT359/data/neuro.txt',header=TRUE,sep="")
nrow(neuro.dat)
library(knitr)
kable(neuro.dat, caption = 'Neuroblastoma Data')
```

In this data, y represents the number of patients that survived two years, and m represents the number of patients with that stage/age pair

```{r}
# age and stage are factors
neuro.dat$age<-as.factor(neuro.dat$age)
neuro.dat$stage<-as.factor(neuro.dat$stage)

# The response for logistic regression consists of a y/m pair
# We construct this here:
neuro.dat$resp<-cbind(neuro.dat$y,neuro.dat$m)
neuro.dat

# fit the logistic model with age and stage and print out summary statistics
model1<-glm(resp ~ age +stage,family=binomial(link=logit), data=neuro.dat)
summary(model1) # note that the first level of each factor is represented by the intercept and is thus baseline level
```


We can also look at the residuals

```{r}
#here we record the deviance residuals, linear predictor, and fitted values
rd1<-residuals.glm(model1,"deviance")
lp1<-model1$linear.predictors
fv1<-model1$fitted.values
plot(fv1,rd1,ylim=c(-3,3),xlab='fitted values',ylab='residuals',pch=11)
abline(h=-2)
abline(h=2)
title('Residuals Plotted Against Fitted Values')
```

How do we formally test whether age or stage are related to survival? We can make models that specifically look at only stage, or only age, and see how the deviance changed from the full model
```{r}
# Here we fit two reduced models to enable us to test the importance of age and stage
model2<- glm(resp ~ age,family=binomial(link=logit), data=neuro.dat)
model3 <- glm(resp ~stage,family=binomial(link=logit), data=neuro.dat)

#Test the significance of stage using a likelihood ratio test
# The test statistic
D.obs<-model2$deviance - model1$deviance
# Under the null hypothesis this came from a chi-squared distn with 4 DOF
1-pchisq(D.obs,4)
anova(model2,model1,test='Chisq')

#Test the significance of age
D.obs<-model3$deviance - model1$deviance
# Under the null hypothesis this came from a chi-squared distn with 2 DOF
1-pchisq(D.obs,2)
# or equivalently
anova(model3,model1,test='Chisq')
```

Both age and stage are significant, though stage is most important. We cannot remove either from the full model.
